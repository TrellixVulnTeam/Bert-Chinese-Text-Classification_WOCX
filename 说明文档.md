# Bert-Chinese-Text-Classification-Pytorch
引用自开源算法：[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE)

中文文本分类，TF-IDF+Bert，基于pytorch。

## 环境
机器：一块1080 ， 训练时间：20分钟。
python 3.6 python 3.7
pytorch
tqdm
sklearn
pandas 
jieba
xlwt
xlrd
xlutils
multiprocessing(多进程处理数据用到)


# 中文数据集
以官方给的训练集数据为基础，添加了从腾讯新闻网站爬取的新闻数据,一共10个类别，每类3200条左右。数据以字为单位输入模型。

类别：财经、房产、教育、科技、军事、汽车、体育、游戏、娱乐、其他。
# 数据集类别划分：
采集到的类别如下
classdict = {'财经':0,'股票':0,'房产':1,'教育':2,'科普': 2,'科技':3,'数码': 3,'军事':4,'汽车':5,'体育':6,'足球': 6,'综合体育最新':6,\
            '体育焦点':6,'游戏':7,'娱乐':8,'其它':9,'其他':9,'社会':9,'健康':9,'法制':9,'世界':9,'国际':9,'文化':9,'历史':9,'时尚':9,'情感':9,\
                '旅游':9,'健康':9,'美食':9,'宠物':9,'星座':9,'动漫':9} 
iddict={0:'财经',1:'房产',2:'教育',3:'科技',4:'军事',5:'汽车',6:'体育',7:'游戏',8:'娱乐',9:'其他'}
把类别用字典映射后，重新划分分类后如下
其中股票归类为财经 把科普归类为教育 数码归类为科技  足球、综合体育和体育焦点归类为体育 社会、健康、法制、世界、国际、文化、历史、时尚、情感、旅游、健康、美食、宠物、星座、动漫等归类为其他

# 数据集划分：
数据集|数据量
--|--
训练集|22590
验证集|3000
测试集|3000

# 代码文件说明
## 数据预处理文件
- xlsxprocess_1.py 
 1).Excel_process(3200).reclass(path_list,save_path)使用pandas读取xlsx或xls文件数据 进行去重、重新划分分类、写入xlsx或xls文件
 2).Excel_process(3200).data_split(save_path) #使用pandas打乱数据集 再使用sklearn的train_test_split把数据集划分为训练集、验证集和测试集并分别写入xlsx或xls文件

- xlsxprocess_2.py
使用pandas读取xlsx或xls文件数据 打乱数据集 对新闻标题和内容进行基于jieba分词和TF-IDF算法的关键词抽取 最后加上标签 写入txt文件
 1).train_path = Excel_testdata().chuli(stopwords,news_train)       # 单进程地对新闻内容处理
 2).train_path = Excel_testdata().multi_chuli(stopwords,news_train) # 多进程地对新闻内容处理

## 训练+测试 主文件
- run.py

## 预测+测试 主文件
- demo.py

## 预训练语言模型文件
bert模型放在 bert_pretain目录下,目录下是三个文件：
 - pytorch_model.bin  
 - bert_config.json  
 - vocab.txt

## 数据集文件
- ./THCNews/data/class.txt 分类表
- ./THCNews/data/train.txt 训练集
- ./THCNews/data/dev.txt 验证集
- ./THCNews/data/test.txt 测试集

## 停用词表
- ./stopwords/cg_stopwords.txt

## 训练保存的权重模型文件
- ./THCNews/saved_dict/bert.ckpt

# 效果

模型|acc|备注
--|--|--
TF-IDF+Bert|自制测试集准确率89.6%|



预训练模型下载地址：  
bert_Chinese: 模型 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz  
              词表 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt  
来自[这里](https://github.com/huggingface/pytorch-transformers)   
备用：模型的网盘地址：https://pan.baidu.com/s/1qSAD5gwClq7xlgzl_4W3Pw

解压后，按照上面说的放在对应目录下，文件名称确认无误即可。  

# 使用说明
## 数据清洗
先运行xlsxprocess_1.py 进行数据去重、重新划分分类  划分训练集、验证集和测试集并分别写入xlsx或xls文件
再运行xlsxprocess_2.py 打乱数据集 对新闻标题和内容进行基于jieba分词和TF-IDF算法的关键词抽取 最后加上标签 写入txt文件

## 训练并测试：
下载好预训练模型
python run.py --model bert

## 预测+测试
python demo.py
- 模式0 结束
- 模式1 逐行预测
- 模式2 excel文件预测
- 模式3 excel文件测试
- 模式4 txt文件测试(目前仅支持清洗后的数据)

### 参数
模型都在models目录下，超参定义和模型定义在同一文件中。  


## 对应论文
[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
